{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating PCA in Pipelines - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous section, you learned about how to use pipelines in scikit-learn to combine several supervised learning algorithms in a manageable pipeline. In this lesson, you will integrate PCA along with classifiers in the pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "In this lab you will: \n",
    "\n",
    "- Integrate PCA in scikit-learn pipelines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data Science Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be following the data science workflow:\n",
    "\n",
    "1. Initial data inspection, exploratory data analysis, and cleaning\n",
    "2. Feature engineering and selection\n",
    "3. Create a baseline model\n",
    "4. Create a machine learning pipeline and compare results with the baseline model\n",
    "5. Interpret the model and draw conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Initial data inspection, exploratory data analysis, and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll use a dataset created by the Otto group, which was also used in a [Kaggle competition](https://www.kaggle.com/c/otto-group-product-classification-challenge/data). The description of the dataset is as follows:\n",
    "\n",
    "The Otto Group is one of the world’s biggest e-commerce companies, with subsidiaries in more than 20 countries, including Crate & Barrel (USA), Otto.de (Germany) and 3 Suisses (France). They are selling millions of products worldwide every day, with several thousand products being added to their product line.\n",
    "\n",
    "A consistent analysis of the performance of their products is crucial. However, due to their global infrastructure, many identical products get classified differently. Therefore, the quality of product analysis depends heavily on the ability to accurately cluster similar products. The better the classification, the more insights the Otto Group can generate about their product range.\n",
    "\n",
    "In this lab, you'll use a dataset containing:\n",
    "- A column `id`, which is an anonymous id unique to a product\n",
    "- 93 columns `feat_1`, `feat_2`, ..., `feat_93`, which are the various features of a product\n",
    "- a column `target` - the class of a product\n",
    "\n",
    "\n",
    "\n",
    "The dataset is stored in the `'otto_group.csv'` file. Import this file into a DataFrame called `data`, and then: \n",
    "\n",
    "- Check for missing values \n",
    "- Check the distribution of columns \n",
    "- ... and any other things that come to your mind to explore the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the dataset\n",
    "data = pd.read_csv(\"otto_group.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Capture .info() output WITHOUT print\n",
    "buffer = io.StringIO()\n",
    "data.info(buf=buffer)\n",
    "info_output = buffer.getvalue()\n",
    "info_output   # This displays the info in Jupyter naturally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Missing values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Statistical summary\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Target distribution\n",
    "data[\"target\"].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.countplot(data=data, x=\"target\")\n",
    "plt.title(\"Target Class Distribution\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Correlation heatmap (sampled)\n",
    "sample = data[feature_cols].sample(2000, random_state=42)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(sample.corr(), cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Heatmap (Sampled)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at all the histograms, you can tell that a lot of the data are zero-inflated, so most of the variables contain mostly zeros and then some higher values here and there. No normality, but for most machine learning techniques this is not an issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Feature histograms\n",
    "feature_cols = [col for col in data.columns if col.startswith(\"feat_\")]\n",
    "data[feature_cols].hist(figsize=(20,20), bins=20)\n",
    "plt.suptitle(\"Feature Distributions\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there are so many zeroes, most values above zero will seem to be outliers. The safe decision for this data is to not delete any outliers and see what happens. With many 0s, sparse data is available and high values may be super informative. Moreover, without having any intuitive meaning for each of the features, we don't know if a value of ~260 is actually an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the features are zero-inflated and we lack domain knowledge,\n",
    "# we will NOT remove any outliers.\n",
    "\n",
    "# This cell intentionally performs no outlier filtering.\n",
    "# We simply continue with the full dataset.\n",
    "\n",
    "data_no_outlier_removal = data.copy()\n",
    "\n",
    "data_no_outlier_removal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering and selection with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the correlation structure of your features using a [heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select feature columns only\n",
    "feature_cols = [col for col in data.columns if col.startswith(\"feat_\")]\n",
    "X = data[feature_cols]\n",
    "\n",
    "# Use a sample to avoid a huge heatmap\n",
    "corr_sample = X.sample(2000, random_state=42)\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(corr_sample.corr(), cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Heatmap of Features (Sampled)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use PCA to select a number of features in a way that you still keep 80% of your explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a sample to avoid a huge heatmap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# PCA keeping 80% variance\n",
    "pca = PCA(n_components=0.8, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Display number of components chosen\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(pca.explained_variance_ratio_.cumsum(), marker=\"o\")\n",
    "plt.axhline(0.80, color=\"red\", linestyle=\"--\")\n",
    "plt.title(\"Cumulative Explained Variance\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a train-test split with a test size of 40%\n",
    "\n",
    "This is a relatively big training set, so you can assign 40% to the test set. Set the `random_state` to 42. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features and target\n",
    "feature_cols = [col for col in data.columns if col.startswith(\"feat_\")]\n",
    "X = data[feature_cols]\n",
    "y = data[\"target\"]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.40,\n",
    "    random_state=42,\n",
    "    stratify=y  # helps maintain class balance\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your baseline model *in a pipeline setting*. In the pipeline: \n",
    "\n",
    "- Your first step will be to scale your features down to the number of features that ensure you keep just 80% of your explained variance (which we saw before)\n",
    "- Your second step will be to build a basic logistic regression model \n",
    "\n",
    "Make sure to fit the model using the training set and test the result by obtaining the accuracy using the test set. Set the `random_state` to 123. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Build the pipeline\n",
    "baseline_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.8, random_state=123)),\n",
    "    ('logreg', LogisticRegression(max_iter=1000, random_state=123))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Fit the model on the training set\n",
    "baseline_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Predict on the test set\n",
    "y_pred_baseline = baseline_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Compute accuracy\n",
    "accuracy_score(y_test, y_pred_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a pipeline consisting of a linear SVM, a simple decision tree, and a simple random forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the above, but now create three different pipelines:\n",
    "- One for a standard linear SVM\n",
    "- One for a default decision tree\n",
    "- One for a random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# ⏰ This cell may take several minutes to run\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.8, random_state=123)),\n",
    "    ('svm', LinearSVC(random_state=123, max_iter=5000))\n",
    "])\n",
    "\n",
    "svm_pipeline.fit(X_train, y_train)\n",
    "y_pred_svm = svm_pipeline.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.8, random_state=123)),\n",
    "    ('tree', DecisionTreeClassifier(random_state=123))\n",
    "])\n",
    "\n",
    "tree_pipeline.fit(X_train, y_train)\n",
    "y_pred_tree = tree_pipeline.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.8, random_state=123)),\n",
    "    ('forest', RandomForestClassifier(random_state=123))\n",
    "])\n",
    "\n",
    "forest_pipeline.fit(X_train, y_train)\n",
    "y_pred_forest = forest_pipeline.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline with grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct two pipelines with grid search:\n",
    "- one for random forests - try to have around 40 different models\n",
    "- one for the AdaBoost algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest pipeline with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here \n",
    "# imports\n",
    "# Your code here \n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# ⏰ This cell may take a long time to run!\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.8, random_state=123)),\n",
    "    ('rf', RandomForestClassifier(random_state=123))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rf = {\n",
    "    'rf__n_estimators': [100, 300],          # 2\n",
    "    'rf__max_depth': [None, 10],             # 2\n",
    "    'rf__min_samples_split': [2, 5],         # 2\n",
    "    'rf__min_samples_leaf': [1, 2],          # 2\n",
    "    'rf__max_features': ['sqrt', 'log2']     # 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your grid search object along with `.cv_results` to get the full result overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here \n",
    "# ⏰ This cell may take a long time to run!\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    estimator=rf_pipeline,\n",
    "    param_grid=param_grid_rf,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results = pd.DataFrame(rf_grid.cv_results_)\n",
    "rf_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# ⏰ This cell may take several minutes to run\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Create pipeline: scaler -> PCA -> AdaBoost\n",
    "ada_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.8, random_state=123)),\n",
    "    ('ada', AdaBoostClassifier(random_state=123))\n",
    "])\n",
    "\n",
    "# Define grid (~40 models)\n",
    "param_grid_ada = {\n",
    "    'ada__n_estimators': [50, 100, 200],\n",
    "    'ada__learning_rate': [0.5, 1.0, 1.5],\n",
    "    'ada__algorithm': ['SAMME', 'SAMME.R']\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "ada_grid = GridSearchCV(\n",
    "    estimator=ada_pipeline,\n",
    "    param_grid=param_grid_ada,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit model (may take several minutes)\n",
    "ada_grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your grid search object along with `.cv_results` to get the full result overview: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here \n",
    "# View full results\n",
    "ada_results = pd.DataFrame(ada_grid.cv_results_)\n",
    "ada_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level-up (Optional): SVM pipeline with grid search \n",
    "\n",
    "As extra level-up work, construct a pipeline with grid search for support vector machines. \n",
    "* Make sure your grid isn't too big. You'll see it takes quite a while to fit SVMs with non-linear kernel functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# ⏰ This cell may take a very long time to run!\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Pipeline: scaler -> PCA -> SVM\n",
    "svm_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.8, random_state=123)),\n",
    "    ('svc', SVC(random_state=123))\n",
    "])\n",
    "\n",
    "# Small parameter grid for speed\n",
    "param_grid_svm = {\n",
    "    'svc__C': [0.1, 1, 10],\n",
    "    'svc__kernel': ['linear', 'rbf'],   # linear + RBF\n",
    "    'svc__gamma': ['scale', 'auto']     # for RBF kernel\n",
    "}\n",
    "\n",
    "svm_grid = GridSearchCV(\n",
    "    estimator=svm_pipeline,\n",
    "    param_grid=param_grid_svm,\n",
    "    cv=3,                # smaller CV for speed\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit model (may take a very long time!)\n",
    "svm_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your grid search object along with `.cv_results` to get the full result overview: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here \n",
    "# SVM full results\n",
    "svm_results = pd.DataFrame(svm_grid.cv_results_)\n",
    "svm_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "Note that this solution is only one of many options. The results in the Random Forest and AdaBoost models show that there is a lot of improvement possible by tuning the hyperparameters further, so make sure to explore this yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "Great! You've gotten a lot of practice in using PCA in pipelines. What algorithm would you choose and why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
